---
sidebar_position: 1
---

# Vision-Language-Action (VLA)

This week, we will cover the basics of Vision-Language-Action (VLA).

## Voice-to-Action using OpenAI Whisper

OpenAI's Whisper is a powerful speech-to-text model that can be used to create a voice-to-action system for your robot. You can use Whisper to transcribe voice commands, and then use a large language model (LLM) to translate those commands into a sequence of actions for your robot.



## Cognitive Planning using LLMs



Large language models (LLMs) can be used for cognitive planning. This means that you can use an LLM to translate a high-level command, such as "Clean the room", into a sequence of low-level actions that your robot can execute. For example, the LLM might generate a plan like this:



1.  Find the trash can.

2.  Pick up any trash on the floor.

3.  Put the trash in the trash can.

4.  Find the vacuum cleaner.

5.  Vacuum the floor.
