"use strict";(globalThis.webpackChunkbook_1=globalThis.webpackChunkbook_1||[]).push([[716],{7506:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter-6","title":"Chapter Six: Sim to Real Transfer","description":"Training in the Matrix: Leveraging simulation for real-world skills.","source":"@site/docs/chapter-6.md","sourceDirName":".","slug":"/chapter-6","permalink":"/docs/chapter-6","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"Chapter Six: Sim to Real Transfer","description":"Training in the Matrix: Leveraging simulation for real-world skills."},"sidebar":"tutorialSidebar","previous":{"title":"Chapter Five: Manipulation and Grasping","permalink":"/docs/chapter-5"},"next":{"title":"Chapter Seven: Reinforcement Learning for Control","permalink":"/docs/chapter-7"}}');var r=i(4848),a=i(8453);const o={sidebar_position:7,title:"Chapter Six: Sim to Real Transfer",description:"Training in the Matrix: Leveraging simulation for real-world skills."},s="Chapter Six: Sim to Real Transfer",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={admonition:"admonition",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-six-sim-to-real-transfer",children:"Chapter Six: Sim to Real Transfer"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Training in the Matrix: Leveraging simulation for real-world skills."})}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Collecting data on real robots is slow, expensive, and dangerous. A robot might fall 10,000 times before it learns to walk. We can't afford that hardware repair bill."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Enter Simulation."}),"\nWe train Reinforcement Learning (RL) agents in simulated worlds (Isaac Lab, Gazebo, Mujoco). Here, we can run thousands of robots in parallel, faster than real-time."]}),"\n",(0,r.jsx)(n.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,r.jsx)(n.p,{children:'To make the policy robust, we randomize everything in the sim: friction, mass, colors, lighting. If the robot learns to walk on "slippery ice" and "sticky mud" in sim, it handles the real office floor easily.'}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Understanding the core concept of ",(0,r.jsx)(n.strong,{children:"Sim to Real Transfer"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Recognizing the challenges in this specific domain."}),"\n",(0,r.jsx)(n.li,{children:"Preparing for the practical application in the next chapter."}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"Reflection",type:"tip",children:(0,r.jsx)(n.p,{children:"Think about how this chapter applies to a robot you might want to build."})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>s});var t=i(6540);const r={},a=t.createContext(r);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);