"use strict";(globalThis.webpackChunkbook_1=globalThis.webpackChunkbook_1||[]).push([[851],{7858:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"chapter-7","title":"Chapter Seven: Reinforcement Learning for Control","description":"Using PPO and reward functions to teach movement.","source":"@site/docs/chapter-7.md","sourceDirName":".","slug":"/chapter-7","permalink":"/ai-driven-hackathon/docs/chapter-7","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Chapter Seven: Reinforcement Learning for Control","description":"Using PPO and reward functions to teach movement."},"sidebar":"tutorialSidebar","previous":{"title":"Chapter Six: Sim to Real Transfer","permalink":"/ai-driven-hackathon/docs/chapter-6"},"next":{"title":"Chapter Eight: Human Robot Interaction HRI","permalink":"/ai-driven-hackathon/docs/chapter-8"}}');var o=t(4848),r=t(8453);const a={sidebar_position:8,title:"Chapter Seven: Reinforcement Learning for Control",description:"Using PPO and reward functions to teach movement."},s="Chapter Seven: Reinforcement Learning for Control",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-seven-reinforcement-learning-for-control",children:"Chapter Seven: Reinforcement Learning for Control"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Using PPO and reward functions to teach movement."})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(n.p,{children:["Instead of writing ",(0,o.jsx)(n.code,{children:"if lean_forward then push_back"}),", we define a ",(0,o.jsx)(n.strong,{children:"Reward Function"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-math",children:"R = R_{velocity} + R_{upright} - C_{energy}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The robot explores actions. If it walks forward and stays upright, it gets points. If it falls or uses too much energy, it loses points. Over millions of iterations, algorithms like ",(0,o.jsx)(n.strong,{children:"PPO (Proximal Policy Optimization)"}),' converge on a walking gait that maximizes reward. This "emergent behavior" often looks surprisingly natural.']}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Understanding the core concept of ",(0,o.jsx)(n.strong,{children:"Reinforcement Learning for Control"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"Recognizing the challenges in this specific domain."}),"\n",(0,o.jsx)(n.li,{children:"Preparing for the practical application in the next chapter."}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{title:"Reflection",type:"tip",children:(0,o.jsx)(n.p,{children:"Think about how this chapter applies to a robot you might want to build."})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var i=t(6540);const o={},r=i.createContext(o);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);