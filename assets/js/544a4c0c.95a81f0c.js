"use strict";(globalThis.webpackChunkbook_1=globalThis.webpackChunkbook_1||[]).push([[997],{7237:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapter-3","title":"Chapter Three: Perception and Vision","description":"How robots see and map their environment.","source":"@site/docs/chapter-3.md","sourceDirName":".","slug":"/chapter-3","permalink":"/docs/chapter-3","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter Three: Perception and Vision","description":"How robots see and map their environment."},"sidebar":"tutorialSidebar","previous":{"title":"Chapter Two: Anatomy of a Humanoid","permalink":"/docs/chapter-2"},"next":{"title":"Chapter Four: Locomotion and Balance","permalink":"/docs/chapter-4"}}');var o=t(4848),a=t(8453);const r={sidebar_position:4,title:"Chapter Three: Perception and Vision",description:"How robots see and map their environment."},s="Chapter Three: Perception and Vision",c={},d=[{value:"Introduction",id:"introduction",level:2},{value:"SLAM (Simultaneous Localization and Mapping)",id:"slam-simultaneous-localization-and-mapping",level:3},{value:"Semantic Understanding",id:"semantic-understanding",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2}];function l(e){const n={admonition:"admonition",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-three-perception-and-vision",children:"Chapter Three: Perception and Vision"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"How robots see and map their environment."})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Vision is the primary sense for a humanoid. But a camera feed is just a 2D array of pixels. The robot needs to turn that into a 3D understanding of the world."}),"\n",(0,o.jsx)(n.h3,{id:"slam-simultaneous-localization-and-mapping",children:"SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,o.jsx)(n.p,{children:"The robot must know where it is (Localization) and what the world looks like (Mapping) at the same time. Modern VSLAM uses visual features to build sparse or dense maps."}),"\n",(0,o.jsx)(n.h3,{id:"semantic-understanding",children:"Semantic Understanding"}),"\n",(0,o.jsx)(n.p,{children:'It\'s not enough to know "there is an object at (x,y,z)". The robot must know "that is a chair, and I can sit on it." This is where Vision-Language Models (VLMs) come into play, allowing us to query the scene.'}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Understanding the core concept of ",(0,o.jsx)(n.strong,{children:"Perception and Vision"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"Recognizing the challenges in this specific domain."}),"\n",(0,o.jsx)(n.li,{children:"Preparing for the practical application in the next chapter."}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{title:"Reflection",type:"tip",children:(0,o.jsx)(n.p,{children:"Think about how this chapter applies to a robot you might want to build."})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);